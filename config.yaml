wandb:
  entity: lloyd-poke
  project: pufferpoke
  group: ~

debug:
  env:
    headless: False
    stream_wrapper: False
    init_state: "victory_road_5"
    state_dir: pyboy_states
    max_steps: 20480
    log_frequency: 1
    disable_ai_actions: True
    use_global_map: False
    reduce_res: True
    animate_scripts: True
    save_state: False
    auto_next_elevator_floor: True 
    auto_solve_strength_puzzles: True
  train:
    device: cpu
    compile: False
    compile_mode: default
    num_envs: 1
    envs_per_worker: 1
    num_workers: 1
    env_batch_size: 128
    zero_copy: False
    batch_size: 1024
    minibatch_size: 128
    batch_rows: 4
    bptt_horizon: 2
    total_timesteps: 1_000_000
    save_checkpoint: True
    checkpoint_interval: 4
    save_overlay: True
    overlay_interval: 1
    verbose: False
    env_pool: False
    load_optimizer_state: False
    async_wrapper: False
    sqlite_wrapper: True
    archive_states: False

env:
  headless: False
  disable_ai_actions: False
  save_final_state: True
  print_rewards: True
  video_dir: video
  state_dir: pyboy_states
  init_state: Bulbasaur
  action_freq: 24
  max_steps: 20480
  save_video: False
  fast_video: False
  frame_stacks: 1
  perfect_ivs: True
  reduce_res: True
  two_bit: True
  log_frequency: 2000
  exploration_inc: 1.0
  exploration_max: 1.0
  max_steps_scaling: 0 # 0.2 # every 10 events or items gained, multiply max_steps by 2
  map_id_scalefactor: 5.0 # multiply map ids whose events have not been completed by 5
  save_state: True
  #Scripts
  disable_wild_encounters: 
    - INDIGO_PLATEAU
  auto_flash: True
  auto_teach_cut: True
  auto_teach_surf: True
  auto_teach_strength: True
  auto_use_cut: True
  auto_use_surf: True
  auto_use_strength: True
  auto_solve_strength_puzzles: True
  auto_remove_all_nonuseful_items: True #data/items.py
  auto_pokeflute: True
  auto_next_elevator_floor: True #Automates elevator use; massive timesaver
  skip_safari_zone: True #TODO: Issue #8 - crashes if set to false 
  insert_saffron_guard_drinks: True #Automates purchasing of drink from vending machine; massive timesaver
  infinite_money: True
  use_global_map: False
  animate_scripts: False

train:
  seed: 1
  torch_deterministic: True
  device: cuda
  compile: True
  compile_mode: "reduce-overhead"
  float32_matmul_precision: "high"
  total_timesteps: 1_000_000_000 # 100_000_000_000 for full games
  batch_size: 65536 
  minibatch_size: 2048
  learning_rate: 2.0e-4
  anneal_lr: False
  gamma: 0.998
  gae_lambda: 0.95
  num_minibatches: 4
  update_epochs: 3
  norm_adv: True
  clip_coef: 0.1
  clip_vloss: True
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: ~
  batch_rows: 128
  bptt_horizon: 16 
  vf_clip_coef: 0.1

  num_envs: 288
  num_workers: 24
  env_batch_size: 36
  env_pool: True
  zero_copy: False

  verbose: True
  data_dir: runs
  save_checkpoint: False
  checkpoint_interval: 200
  save_overlay: True
  overlay_interval: 100
  cpu_offload: False
  pool_kernel: [0]
  load_optimizer_state: False
  use_rnn: True
  async_wrapper: False
  sqlite_wrapper: True
  archive_states: True
  swarm: True
  early_stop:
    # event name: minutes. If we dont satisfy each condition
    # we early stop
    # The defaults have a margin of error
    EVENT_BEAT_BROCK: 30
    EVENT_BEAT_MISTY: 90
    EVENT_GOT_HM01: 180
  one_epoch: "EVENT_BEAT_CHAMPION_RIVAL"

wrappers:
  empty:
    - episode_stats.EpisodeStatsWrapper: {}

  baseline:
    - stream_wrapper.StreamWrapper:
        user: lloyd
    - exploration.DecayWrapper:
        step_forgetting_factor:
          npc: 0.995
          coords: 0.9995
          map_ids: 0.995
          explore: 0.9995
          start_menu: 0.998
          pokemon_menu: 0.998
          stats_menu: 0.998
          bag_menu: 0.998
          action_bag_menu: 0.998
        forgetting_frequency: 10
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 0
  
  finite_coords:
    - stream_wrapper.StreamWrapper:
        user: lloyd
    - exploration.MaxLengthWrapper:
        capacity: 1750
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 0

  stream_only:
    - stream_wrapper.StreamWrapper:
        user: lloyd 
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 1
        jitter: 1

  fixed_reset_value:
    - stream_wrapper.StreamWrapper:
        user: lloyd 
    - exploration.OnResetLowerToFixedValueWrapper:
        fixed_value:
          coords: 0.33
          map_ids: 0.33
          npc: 0.33
          cut: 0.33
          explore: 0.33
    - exploration.OnResetExplorationWrapper:
        full_reset_frequency: 25
        jitter: 0
    - episode_stats.EpisodeStatsWrapper: {}

#Weights to assign different rewards. 0 to disable rewarding a particular activity
rewards:
  event-only:
    reward:
      event: 1.0 #Any event completion (see data/events.py)
  baseline:
    reward:
      event: 1.0 #Any event completion (see data/events.py)
      required_event: 5.0 #Events required to progress the game (see data/events.py)
      required_item: 5.0 #Items (keys/HMs/etc) required for game progress (see data/items.py)
      useful_item: 1.0 #(see data/items.py)
      seen_pokemon: 4.0
      caught_pokemon: 4.0
      moves_obtained: 4.0 #TM/HM moves taught to party pokemon
      hm_count: 10.0
      max_party_level: 1.0 #Highest sum of party pokemon levels
      max_opponent_level: 0.0 #Highest enemy pokemon level encountered
      death_reward: 0.0
      blackout_check: 0.0
      badges: 5.0
      cut_coords: 0.0 #Weighted location coords cut has been used (+10 correct, +.001 incorrect)
      cut_tiles: 0.0 #Number of different tiles cut has been used on
      start_menu: 0.0
      pokemon_menu: 0.0
      stats_menu: 0.0
      bag_menu: 0.0
      seen_action_bag_menu: 0.0
      exploration: 0.02
      explore_npcs: 0.0
      explore_hidden_objs: 0.0001
      explore_signs: 0.015
      pokecenter_heal: 0.5
      a_press: 0.0 # 0.00001
      explore_warps: 0.01
      use_surf: 0.5 #If surf has ever been used

policies:
  multi_convolutional.MultiConvolutionalPolicy:
    policy:
      hidden_size: 512

    rnn:
      # Assumed to be in the same module as the policy
      name: MultiConvolutionalRNN
      args:
        input_size: 512
        hidden_size: 512
        num_layers: 1
